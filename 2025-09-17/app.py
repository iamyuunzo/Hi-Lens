import streamlit as st
import pdfplumber
import os
import tempfile
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
import re
from typing import List, Dict
import numpy as np
import pandas as pd

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="PDF ë¬¸ì„œ ë¶„ì„ AI v2",
    page_icon="ğŸ“Š",
    layout="wide"
)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'embedding_model' not in st.session_state:
    st.session_state.embedding_model = None
if 'documents_loaded' not in st.session_state:
    st.session_state.documents_loaded = False
if 'chunks_data' not in st.session_state:
    st.session_state.chunks_data = []
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = []
if 'page_mapping' not in st.session_state:
    st.session_state.page_mapping = {}

class PDFAnalyzerV2:
    def __init__(self):
        self.embedding_model = None
        self.chunks_data = []
        self.embeddings = []
        self.page_mapping = {}
        
    def initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        if self.embedding_model is None:
            with st.spinner("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©ì¤‘ì…ë‹ˆë‹¤..."):
                self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        return True
    
    def extract_real_page_numbers(self, pdf_path):
        """ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ"""
        page_mapping = {}
        
        try:
            with pdfplumber.open(pdf_path) as pdf:
                for i, page in enumerate(pdf.pages):
                    text = page.extract_text() or ""
                    
                    # í˜ì´ì§€ í•˜ë‹¨ì—ì„œ ë²ˆí˜¸ ì°¾ê¸°
                    lines = text.split('\n')
                    bottom_lines = lines[-3:]  # ë§ˆì§€ë§‰ 3ì¤„ í™•ì¸
                    
                    real_page_num = None
                    for line in bottom_lines:
                        line = line.strip()
                        # ë‹¤ì–‘í•œ í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´
                        patterns = [
                            r'-\s*(\d+)\s*-',      # - 46 -
                            r'^\s*(\d+)\s*$',      # 46
                            r'Page\s*(\d+)',       # Page 46
                            r'(\d+)\s*í˜ì´ì§€',      # 46í˜ì´ì§€
                            r'(\d+)\s*/\s*\d+',    # 46/100
                        ]
                        
                        for pattern in patterns:
                            match = re.search(pattern, line)
                            if match:
                                real_page_num = int(match.group(1))
                                break
                        if real_page_num:
                            break
                    
                    # ì‹¤ì œ ë²ˆí˜¸ë¥¼ ì°¾ì§€ ëª»í•˜ë©´ ìˆœì„œ ë²ˆí˜¸ ì‚¬ìš©
                    if real_page_num is None:
                        real_page_num = i + 1
                    
                    page_mapping[i + 1] = real_page_num
                    
        except Exception as e:
            st.warning(f"í˜ì´ì§€ ë²ˆí˜¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            # ê¸°ë³¸ ë§¤í•‘ (ìˆœì„œëŒ€ë¡œ)
            return {i+1: i+1 for i in range(100)}
        
        return page_mapping
    
    def extract_text_and_tables_from_pdf(self, pdf_file):
        """PDFì—ì„œ í…ìŠ¤íŠ¸ì™€ í‘œ ëª¨ë‘ ì¶”ì¶œ (ê°œì„ ëœ ë²„ì „)"""
        pages_data = []
        
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(pdf_file.read())
                tmp_file_path = tmp_file.name
            
            # ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ ë§¤í•‘ ìƒì„±
            self.page_mapping = self.extract_real_page_numbers(tmp_file_path)
            st.session_state.page_mapping = self.page_mapping
            
            with pdfplumber.open(tmp_file_path) as pdf:
                for page_index, page in enumerate(pdf.pages):
                    sequence_num = page_index + 1
                    real_page_num = self.page_mapping.get(sequence_num, sequence_num)
                    
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    page_text = page.extract_text() or ""
                    
                    # í‘œ ì¶”ì¶œ ë° ì²˜ë¦¬
                    tables = page.extract_tables()
                    table_text = ""
                    
                    if tables:
                        for table_idx, table in enumerate(tables):
                            table_text += f"\n\n[í‘œ {table_idx + 1} - í˜ì´ì§€ {real_page_num}]\n"
                            
                            # í‘œ ë°ì´í„° ì²˜ë¦¬ (merge cell ê³ ë ¤)
                            processed_table = self.process_table_data(table)
                            
                            # í‘œë¥¼ êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                            for row_idx, row in enumerate(processed_table):
                                if row_idx == 0:  # í—¤ë”
                                    table_text += "| " + " | ".join(str(cell) for cell in row) + " |\n"
                                    table_text += "|" + "|".join("---" for _ in row) + "|\n"
                                else:  # ë°ì´í„° í–‰
                                    table_text += "| " + " | ".join(str(cell) for cell in row) + " |\n"
                    
                    combined_text = page_text + table_text
                    
                    if combined_text.strip():
                        pages_data.append({
                            'sequence_num': sequence_num,
                            'real_page_num': real_page_num,
                            'text': combined_text.strip(),
                            'has_tables': len(tables) > 0,
                            'table_count': len(tables)
                        })
            
            os.unlink(tmp_file_path)
            return pages_data
            
        except Exception as e:
            st.error(f"PDF ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def process_table_data(self, table):
        """í‘œ ë°ì´í„° ì²˜ë¦¬ - merge cell ë° ë¹ˆ ì…€ ì²˜ë¦¬"""
        processed_table = []
        
        for row_idx, row in enumerate(table):
            processed_row = []
            prev_cell_value = ""
            
            for cell_idx, cell in enumerate(row):
                if cell is None or str(cell).strip() == "":
                    # ë¹ˆ ì…€ ì²˜ë¦¬
                    if cell_idx > 0 and prev_cell_value:
                        # ì´ì „ ì…€ê³¼ ë³‘í•©ëœ ê²ƒìœ¼ë¡œ ì¶”ì •
                        processed_row.append(f"[{prev_cell_value}]")
                    else:
                        processed_row.append("[ë¹ˆì…€]")
                else:
                    cell_value = str(cell).strip()
                    processed_row.append(cell_value)
                    prev_cell_value = cell_value
            
            processed_table.append(processed_row)
        
        return processed_table
    
    def smart_text_chunking_v2(self, pages_data: List[Dict], chunk_size: int = 1500, overlap: int = 300):
        """ê°œì„ ëœ ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ì²­í‚¹ - í‘œ ì œëª©ê³¼ ë‚´ìš© ë³´ì¡´"""
        chunks = []
        
        for page_data in pages_data:
            real_page_num = page_data['real_page_num']
            text = page_data['text']
            
            # í‘œ ì œëª© íŒ¨í„´ ê°ì§€ ë° ë³´ì¡´
            table_sections = self.identify_table_sections(text)
            
            if table_sections:
                # í‘œê°€ ìˆëŠ” ê²½ìš° - í‘œ ë‹¨ìœ„ë¡œ ì²­í‚¹
                for section in table_sections:
                    if len(section) > chunk_size:
                        # í° í‘œëŠ” ì ì ˆíˆ ë¶„í• 
                        sub_chunks = self.split_large_section(section, chunk_size, overlap)
                        for sub_chunk in sub_chunks:
                            chunks.append({
                                'text': sub_chunk,
                                'real_page_num': real_page_num,
                                'chunk_id': len(chunks),
                                'has_table': True
                            })
                    else:
                        chunks.append({
                            'text': section,
                            'real_page_num': real_page_num,
                            'chunk_id': len(chunks),
                            'has_table': True
                        })
            else:
                # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹
                text_chunks = self.split_text_by_sentences(text, chunk_size, overlap)
                for chunk_text in text_chunks:
                    chunks.append({
                        'text': chunk_text,
                        'real_page_num': real_page_num,
                        'chunk_id': len(chunks),
                        'has_table': False
                    })
        
        return chunks
    
    def identify_table_sections(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ í‘œ ì„¹ì…˜ ì‹ë³„"""
        sections = []
        
        # í‘œ ì œëª© íŒ¨í„´ë“¤
        table_patterns = [
            r'<[^>]+>.*?(?=\n\n|\n<|$)',  # <ì œëª©> í˜•íƒœ
            r'\[í‘œ[^\]]*\].*?(?=\n\n|\n\[|$)',  # [í‘œ X] í˜•íƒœ
            r'í‘œ\s*\d+.*?(?=\n\n|í‘œ\s*\d+|$)',  # í‘œ 1, í‘œ 2 í˜•íƒœ
        ]
        
        for pattern in table_patterns:
            matches = re.finditer(pattern, text, re.DOTALL)
            for match in matches:
                section = match.group().strip()
                if len(section) > 50:  # ë„ˆë¬´ ì§§ì€ ê±´ ì œì™¸
                    sections.append(section)
        
        return sections
    
    def split_large_section(self, section, chunk_size, overlap):
        """í° ì„¹ì…˜ì„ ì ì ˆíˆ ë¶„í• """
        if len(section) <= chunk_size:
            return [section]
        
        chunks = []
        start = 0
        
        while start < len(section):
            end = start + chunk_size
            
            if end >= len(section):
                chunks.append(section[start:])
                break
            
            # ì ì ˆí•œ ë¶„í•  ì§€ì  ì°¾ê¸° (ì¤„ë°”ê¿ˆ ê¸°ì¤€)
            split_point = section.rfind('\n', start, end)
            if split_point == -1:
                split_point = end
            
            chunks.append(section[start:split_point])
            start = split_point - overlap
            
        return chunks
    
    def split_text_by_sentences(self, text, chunk_size, overlap):
        """ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• """
        sentences = re.split(r'(?<=[.!?])\s+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) > chunk_size and current_chunk:
                chunks.append(current_chunk.strip())
                # ì˜¤ë²„ë© ì²˜ë¦¬
                overlap_sentences = current_chunk.split('.')[-2:]
                current_chunk = '. '.join(overlap_sentences).strip() + '. ' + sentence
            else:
                current_chunk += ' ' + sentence
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def create_embeddings(self, chunks: List[Dict], filename: str):
        """ì„ë² ë”© ìƒì„± ë° ì €ì¥"""
        try:
            self.chunks_data = chunks
            self.embeddings = []
            
            with st.spinner(f"ë¬¸ì„œë¥¼ ë²¡í„°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤... ({len(chunks)}ê°œ ì²­í¬)"):
                progress_bar = st.progress(0)
                
                for i, chunk_data in enumerate(chunks):
                    chunk_text = chunk_data['text']
                    embedding = self.embedding_model.encode([chunk_text])[0]
                    self.embeddings.append(embedding)
                    
                    progress_bar.progress((i + 1) / len(chunks))
                
                progress_bar.empty()
            
            st.session_state.chunks_data = self.chunks_data
            st.session_state.embeddings = self.embeddings
            
            return True
            
        except Exception as e:
            st.error(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return False
    
    def cosine_similarity(self, a, b):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
    
    def hybrid_search_v2(self, query: str, n_results: int = 10) -> List[Dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ - í‚¤ì›Œë“œ + ì˜ë¯¸ ê²€ìƒ‰ + í‘œ íŠ¹í™”"""
        if not self.chunks_data or not self.embeddings:
            return []
        
        try:
            # 1. í‘œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€
            is_table_query = self.detect_table_query(query)
            
            # 2. ì˜ë¯¸ ê²€ìƒ‰
            query_embedding = self.embedding_model.encode([query])[0]
            semantic_scores = []
            
            for i, embedding in enumerate(self.embeddings):
                similarity = self.cosine_similarity(query_embedding, embedding)
                semantic_scores.append(similarity)
            
            # 3. í‚¤ì›Œë“œ ê²€ìƒ‰
            keyword_scores = self.calculate_keyword_scores(query)
            
            # 4. ì ìˆ˜ ê²°í•© ë° ê°€ì¤‘ì¹˜ ì ìš©
            combined_results = []
            for i in range(len(self.chunks_data)):
                chunk_data = self.chunks_data[i]
                
                # ê¸°ë³¸ ì ìˆ˜ ê²°í•©
                semantic_weight = 0.6
                keyword_weight = 0.4
                
                # í‘œ ê´€ë ¨ ì§ˆë¬¸ì´ë©´ í‘œê°€ ìˆëŠ” ì²­í¬ì— ë³´ë„ˆìŠ¤
                table_bonus = 0
                if is_table_query and chunk_data.get('has_table', False):
                    table_bonus = 0.2
                
                # ìµœì¢… ì ìˆ˜
                final_score = (semantic_scores[i] * semantic_weight + 
                             keyword_scores[i] * keyword_weight + 
                             table_bonus)
                
                combined_results.append({
                    'index': i,
                    'final_score': final_score,
                    'semantic_score': semantic_scores[i],
                    'keyword_score': keyword_scores[i],
                    'table_bonus': table_bonus,
                    'chunk_data': chunk_data
                })
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            combined_results.sort(key=lambda x: x['final_score'], reverse=True)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for i, item in enumerate(combined_results[:n_results]):
                chunk_data = item['chunk_data']
                results.append({
                    'text': chunk_data['text'],
                    'real_page_num': chunk_data['real_page_num'],
                    'chunk_id': chunk_data['chunk_id'],
                    'has_table': chunk_data.get('has_table', False),
                    'similarity_score': item['final_score'],
                    'semantic_score': item['semantic_score'],
                    'keyword_score': item['keyword_score'],
                    'table_bonus': item['table_bonus'],
                    'rank': i + 1
                })
            
            return results
            
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []
    
    def detect_table_query(self, query):
        """í‘œ ê´€ë ¨ ì§ˆë¬¸ ê°ì§€"""
        table_keywords = [
            'í‘œ', 'ë„í‘œ', 'ì°¨íŠ¸', 'ê·¸ë˜í”„', 'ë°ì´í„°', 'ìˆ˜ì¹˜', 'í†µê³„',
            'ê²°ê³¼', 'í˜„í™©', 'ë¹„êµ', 'ë¶„ì„', 'ì •ë¦¬', 'ì—‘ì…€', 'ì‹œíŠ¸'
        ]
        query_lower = query.lower()
        return any(keyword in query_lower for keyword in table_keywords)
    
    def calculate_keyword_scores(self, query):
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        query_lower = query.lower()
        query_words = [word for word in query_lower.split() if len(word) > 1]
        
        keyword_scores = []
        for chunk_data in self.chunks_data:
            text_lower = chunk_data['text'].lower()
            
            score = 0
            for word in query_words:
                # ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­
                if word in text_lower:
                    score += text_lower.count(word) * 2
                
                # ë¶€ë¶„ ë§¤ì¹­
                for text_word in text_lower.split():
                    if word in text_word and len(word) > 2:
                        score += 0.5
            
            # ì •ê·œí™”
            max_possible = len(query_words) * 10
            normalized_score = min(score / max_possible, 1.0) if max_possible > 0 else 0
            keyword_scores.append(normalized_score)
        
        return keyword_scores
    
    def generate_contextual_answer(self, question: str, search_results: List[Dict], api_key: str):
        """ë§¥ë½ì„ ê³ ë ¤í•œ ë‹µë³€ ìƒì„± (í˜ì´ì§€ ë²ˆí˜¸ ì •í™•ì„± ê°œì„ )"""
        if not search_results:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
        
        try:
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-1.5-pro-latest')
            
            # í˜ì´ì§€ë³„ë¡œ ê²°ê³¼ ê·¸ë£¹í™” (ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ ê¸°ì¤€)
            page_groups = {}
            for result in search_results:
                page_num = result['real_page_num']
                if page_num not in page_groups:
                    page_groups[page_num] = []
                page_groups[page_num].append(result)
            
            context_parts = []
            source_info = []
            
            for page_num in sorted(page_groups.keys()):
                page_results = page_groups[page_num]
                page_results.sort(key=lambda x: x['similarity_score'], reverse=True)
                
                for i, result in enumerate(page_results[:2]):
                    excerpt_num = len(context_parts) + 1
                    context_parts.append(f"[í˜ì´ì§€ {page_num}, ë°œì·Œ {excerpt_num}]\n{result['text']}")
                    source_info.append({
                        'real_page_num': page_num,
                        'text': result['text'],
                        'similarity_score': result['similarity_score'],
                        'has_table': result['has_table'],
                        'excerpt_num': excerpt_num
                    })
            
            context = "\n\n".join(context_parts)
            
            prompt = f"""
ë‹¤ìŒì€ PDF ë¬¸ì„œì—ì„œ ë°œì·Œí•œ ë‚´ìš©ë“¤ì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {question}

ì§€ì‹œì‚¬í•­:
1. ìœ„ ë¬¸ì„œ ë°œì·Œ ë‚´ìš©ì—ì„œë§Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ì€ ë°˜ë“œì‹œ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì¸ìš©í•˜ì„¸ìš”. ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ë°”ê¿”ì“°ì§€ ë§ˆì„¸ìš”.
3. ì—¬ëŸ¬ ë°œì·Œ ë‚´ìš©ì´ ê´€ë ¨ìˆë‹¤ë©´ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”.
4. ë¬¸ì„œì— í•´ë‹¹ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
5. ë‹µë³€í•  ë•Œ ë°˜ë“œì‹œ ì •í™•í•œ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ëª…ì‹œí•˜ì„¸ìš”: [í˜ì´ì§€ X, ë°œì·Œ Y]
6. í‘œ ë°ì´í„°ì˜ ê²½ìš° êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ì—¬ ì¸ìš©í•˜ì„¸ìš”.
7. ìˆ«ì, ë‚ ì§œ, ê³ ìœ ëª…ì‚¬ëŠ” ì›ë¬¸ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
8. í‘œë¥¼ ì—‘ì…€ í˜•íƒœë¡œ ì •ë¦¬í•˜ë¼ëŠ” ìš”ì²­ì´ë©´ | êµ¬ë¶„ìë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë¦¬í•˜ì„¸ìš”.
"""
            
            response = model.generate_content(prompt)
            return response.text, source_info
            
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", []

# ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
def main():
    st.title("ğŸ“Š PDF ë¬¸ì„œ ë¶„ì„ AI v2")
    st.markdown("**ğŸ†• ê°œì„ ì‚¬í•­:** ì •í™•í•œ í˜ì´ì§€ ë²ˆí˜¸ | í‘œ ì²˜ë¦¬ ê°•í™” | í‚¤ì›Œë“œ ê²€ìƒ‰ ê°œì„ ")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        api_key = st.text_input("Google Gemini API í‚¤", type="password", 
                               help="https://aistudio.google.com/app/apikey ì—ì„œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”")
        
        if not api_key:
            st.warning("API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        
        st.markdown("---")
        st.markdown("### ğŸ†• v2 ê°œì„ ì‚¬í•­")
        st.markdown("""
        - âœ… **ì •í™•í•œ í˜ì´ì§€ ë²ˆí˜¸** (ë¬¸ì„œ í•˜ë‹¨ ê¸°ì¤€)
        - âœ… **í‚¤ì›Œë“œ ê²€ìƒ‰ ê°•í™”** (í˜ì´ì§€ ë²ˆí˜¸ ì—†ì´ë„ ê²€ìƒ‰)
        - âœ… **í‘œ ì²˜ë¦¬ ê°œì„ ** (merge cell ì§€ì›)
        - âœ… **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰** (ì˜ë¯¸+í‚¤ì›Œë“œ)
        """)
        
        st.markdown("---")
        st.markdown("### ğŸ“– ì‚¬ìš©ë²•")
        st.markdown("""
        1. Google Gemini API í‚¤ ì…ë ¥
        2. PDF íŒŒì¼ ì—…ë¡œë“œ
        3. ë¬¸ì„œ ì²˜ë¦¬ ëŒ€ê¸°
        4. ì§ˆë¬¸ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰
        """)
    
    # PDF ë¶„ì„ê¸° ì´ˆê¸°í™”
    if 'analyzer' not in st.session_state:
        st.session_state.analyzer = PDFAnalyzerV2()
    
    analyzer = st.session_state.analyzer
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    if not st.session_state.get('models_initialized', False):
        if analyzer.initialize_models():
            st.session_state.models_initialized = True
            st.session_state.embedding_model = analyzer.embedding_model
    
    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['pdf'])
    
    if uploaded_file is not None:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.info(f"ğŸ“ ì—…ë¡œë“œëœ íŒŒì¼: {uploaded_file.name}")
        
        with col2:
            process_button = st.button("ğŸš€ v2 ì²˜ë¦¬ ì‹œì‘", type="primary")
        
        if process_button and api_key:
            with st.spinner("PDF ë¬¸ì„œë¥¼ v2 ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # í…ìŠ¤íŠ¸ ë° í‘œ ì¶”ì¶œ
                st.write("ğŸ” í…ìŠ¤íŠ¸ ë° í‘œ ì¶”ì¶œ ì¤‘...")
                pages_data = analyzer.extract_text_and_tables_from_pdf(uploaded_file)
                
                if pages_data:
                    total_chars = sum(len(page['text']) for page in pages_data)
                    table_pages = sum(1 for page in pages_data if page['has_tables'])
                    
                    st.success(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(pages_data)}í˜ì´ì§€, {total_chars:,}ê¸€ì, í‘œ í¬í•¨ í˜ì´ì§€: {table_pages}ê°œ")
                    
                    # í˜ì´ì§€ ë§¤í•‘ ì •ë³´ í‘œì‹œ
                    if st.session_state.page_mapping:
                        mapping_info = []
                        for seq, real in list(st.session_state.page_mapping.items())[:5]:
                            mapping_info.append(f"ìˆœì„œ{seq}â†’ì‹¤ì œ{real}")
                        st.info(f"ğŸ“„ í˜ì´ì§€ ë§¤í•‘ ì˜ˆì‹œ: {', '.join(mapping_info)}...")
                    
                    # ìŠ¤ë§ˆíŠ¸ ì²­í‚¹
                    st.write("ğŸ§  ìŠ¤ë§ˆíŠ¸ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
                    chunks = analyzer.smart_text_chunking_v2(pages_data)
                    table_chunks = sum(1 for chunk in chunks if chunk.get('has_table', False))
                    
                    st.success(f"âœ… ë¶„í•  ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ (í‘œ í¬í•¨: {table_chunks}ê°œ)")
                    
                    # ì„ë² ë”© ìƒì„±
                    st.write("ğŸ—„ï¸ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì¤‘...")
                    if analyzer.create_embeddings(chunks, uploaded_file.name):
                        st.success("ğŸ‰ v2 ì²˜ë¦¬ ì™„ë£Œ! ì´ì œ ê°œì„ ëœ ê²€ìƒ‰ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                        st.session_state.documents_loaded = True
                    else:
                        st.error("âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
                else:
                    st.error("âŒ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
    
    # ì§ˆë¬¸ ì„¹ì…˜
    if st.session_state.documents_loaded and api_key:
        st.markdown("---")
        st.header("ğŸ’¬ ê°œì„ ëœ ë¬¸ì„œ ê²€ìƒ‰")
        
        # ì˜ˆì‹œ ì§ˆë¬¸ë“¤
        st.markdown("**ğŸ’¡ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ì˜ˆì‹œ:**")
        example_questions = [
            "ì „êµ­ ê¸°ì¤€ ì„¤ë¹„ì˜ˆë¹„ìœ¨ ì‚°ì¶œ ê²°ê³¼ í‘œ ì •ë¦¬í•´ì¤˜",  # í˜ì´ì§€ ë²ˆí˜¸ ì—†ì´
            "46ìª½ í‘œë¥¼ ì—‘ì…€ìš©ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜",  # í˜ì´ì§€ ë²ˆí˜¸ ìˆì´
            "ì£¼ìš” ì¬ë¬´ ì§€í‘œëŠ” ë¬´ì—‡ì¸ê°€?",
            "ë¦¬ìŠ¤í¬ ìš”ì¸ ë¶„ì„",
            "í–¥í›„ ì „ë§ ë° ê³„íš"
        ]
        
        cols = st.columns(2)
        for i, question in enumerate(example_questions):
            with cols[i % 2]:
                if st.button(question, key=f"example_{i}"):
                    st.session_state.current_question = question
        
        # ì§ˆë¬¸ ì…ë ¥
        question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:", 
                                value=st.session_state.get('current_question', ''),
                                placeholder="ì˜ˆ: ì„¤ë¹„ì˜ˆë¹„ìœ¨ í‘œë¥¼ ì—‘ì…€ í˜•íƒœë¡œ ì •ë¦¬í•´ì¤˜")
        
        col1, col2 = st.columns([3, 1])
        with col1:
            search_button = st.button("ğŸ” v2 ê²€ìƒ‰", type="primary")
        with col2:
            clear_button = st.button("ğŸ—‘ï¸ ì´ˆê¸°í™”")
        
        if clear_button:
            st.session_state.current_question = ""
            st.rerun()
        
        if search_button and question:
            with st.spinner("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìœ¼ë¡œ ìµœì ì˜ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                # ì„¸ì…˜ì—ì„œ ë°ì´í„° ë³µì›
                analyzer.chunks_data = st.session_state.chunks_data
                analyzer.embeddings = st.session_state.embeddings
                analyzer.page_mapping = st.session_state.page_mapping
                
                # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
                search_results = analyzer.hybrid_search_v2(question, n_results=10)
                
                if search_results:
                    # ë‹µë³€ ìƒì„±
                    answer, source_info = analyzer.generate_contextual_answer(question, search_results, api_key)
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.markdown("### ğŸ“‹ ë‹µë³€")
                    st.markdown(answer)
                    
                    # ì¶œì²˜ ì •ë³´ (ê°œì„ ëœ í˜ì´ì§€ ë²ˆí˜¸)
                    if source_info:
                        st.markdown("### ğŸ“ ì¶œì²˜ ì •ë³´ (ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸)")
                        source_df_data = []
                        for info in source_info:
                            source_df_data.append({
                                "ë°œì·Œ": info['excerpt_num'],
                                "ì‹¤ì œ í˜ì´ì§€": info['real_page_num'],
                                "ê´€ë ¨ì„±": f"{info['similarity_score']:.2%}",
                                "í‘œ í¬í•¨": "âœ…" if info['has_table'] else "âŒ",
                                "ê¸¸ì´": f"{len(info['text'])}ì"
                            })
                        
                        st.dataframe(source_df_data, use_container_width=True)
                    
                    # ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼
                    with st.expander("ğŸ” ìƒì„¸ ê²€ìƒ‰ ê²°ê³¼ ë³´ê¸°"):
                        for i, result in enumerate(search_results[:6]):
                            score_info = (f"ì¢…í•©: {result['similarity_score']:.2%} | "
                                        f"ì˜ë¯¸: {result['semantic_score']:.2%} | "
                                        f"í‚¤ì›Œë“œ: {result['keyword_score']:.2%}")
                            if result['table_bonus'] > 0:
                                score_info += f" | í‘œë³´ë„ˆìŠ¤: +{result['table_bonus']:.1%}"
                            
                            st.markdown(f"**ğŸ“„ ì‹¤ì œ í˜ì´ì§€ {result['real_page_num']} | {score_info}**")
                            st.text_area(f"ë‚´ìš© {i+1}", result['text'], height=150, key=f"result_{i}")
                            st.markdown("---")
                else:
                    st.warning("âš ï¸ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    elif not api_key:
        st.warning("âš ï¸ Google Gemini API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    elif not st.session_state.documents_loaded:
        st.info("ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³  v2 ì²˜ë¦¬ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
    
    # í•˜ë‹¨ ì •ë³´
    st.markdown("---")
    st.markdown("**ğŸ”§ v2 íŠ¹ì§•:** ì‹¤ì œ í˜ì´ì§€ ë²ˆí˜¸ | í‘œ êµ¬ì¡° ë³´ì¡´ | í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ | merge cell ì§€ì›")

if __name__ == "__main__":
    main()
