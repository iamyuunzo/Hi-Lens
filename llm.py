# llm.py â€” GPT(OpenAI) ì „ìš© ë²„ì „
from __future__ import annotations
import os
from typing import Optional, List, Dict, Any, Union

import streamlit as st
from openai import OpenAI

try:
    from PIL import Image as PILImage
except Exception:
    PILImage = None

__all__ = [
    "get_provider_name",
    "SUMMARIZER_DEFAULT_SYSTEM",
    "llm_chat",
    "answer_with_context",
    "explain_tables",
    "explain_figure_image",
]

# -------------------------------------------------------------------
# ğŸ”‘ OpenAI API í‚¤ ê°€ì ¸ì˜¤ê¸°
# -------------------------------------------------------------------
def _get_openai_client() -> OpenAI:
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        st.error("âŒ OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤."); st.stop()
    return OpenAI(api_key=api_key)


def get_provider_name() -> str:
    return "OPENAI"


SUMMARIZER_DEFAULT_SYSTEM = (
    "ë‹¹ì‹ ì€ ì •ì±…/ë³´ê³ ì„œ ì „ë¬¸ ìš”ì•½ê°€ì…ë‹ˆë‹¤. ë¬¸ì„œ ë°– ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ë§ê³  "
    "ëª…í™•í•œ í•œêµ­ì–´ë¡œ í•µì‹¬ì„ ì •ë¦¬í•˜ì„¸ìš”."
)

# -------------------------------------------------------------------
# ğŸ”§ ê³µí†µ LLM í˜¸ì¶œ í•¨ìˆ˜
# -------------------------------------------------------------------
def llm_chat(system_prompt: str, user_prompt: str, model_name: str = "gpt-4o-mini") -> str:
    client = _get_openai_client()
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"âš ï¸ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}"


# -------------------------------------------------------------------
# ğŸ“„ ë¬¸ë§¥ ê¸°ë°˜ ë‹µë³€
# -------------------------------------------------------------------
def answer_with_context(query: str, context: str, page_label: Optional[str] = None,
                        model_name: str = "gpt-4o-mini") -> str:
    client = _get_openai_client()
    page_note = f"(ê·¼ê±° p.{page_label})" if page_label else ""
    prompt = f"""
ì•„ë˜ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”. {page_note}

[ë‹µë³€ ê·œì¹™]
- ë‹µë³€ì€ 3~4ë¬¸ì¥ ì •ë„ì˜ ë‹¨ë½í˜• ìš”ì•½ìœ¼ë¡œ ì‘ì„±
- ë²ˆí˜¸ ë§¤ê¸°ê¸°, ë¶ˆë¦¿ ì‚¬ìš© ê¸ˆì§€
- ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ë§¥ì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”
- ë¶ˆí•„ìš”í•œ <br>, HTML, í‘œëŠ” ê¸ˆì§€

[ë¬¸ë§¥]
{context}

[ì§ˆë¬¸]
{query}
""".strip()
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": SUMMARIZER_DEFAULT_SYSTEM},
                {"role": "user", "content": prompt},
            ],
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"âš ï¸ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}"


# -------------------------------------------------------------------
# ğŸ“Š í‘œ ì„¤ëª…
# -------------------------------------------------------------------
def explain_tables(query: str, tables_ctxs: List[Dict[str, Any]],
                   model_name: str = "gpt-4o-mini") -> str:
    client = _get_openai_client()
    parts = []
    for t in tables_ctxs:
        title = (t.get("title") or "").strip()
        pno   = t.get("page_label", "?")
        prev  = (t.get("preview_md") or "").strip()
        nb    = (t.get("neighbor_text") or "").strip()
        sparse = len(prev) < 40 or prev.count("|") <= 2
        ctx_block = f"(p.{pno}) {title}\n"
        if sparse and nb:
            ctx_block += f"[í‘œ ë¯¸ë¦¬ë³´ê¸° ì¶”ì •ì´ ë¹ˆì•½í•˜ì—¬ ë³¸ë¬¸ ë³´ê°•]\n{nb}\n"
        ctx_block += (prev if prev else "")
        parts.append(ctx_block)

    ctx = ("\n\n---\n\n".join(parts))[:7800] if parts else "í‘œ ë¯¸ê²€ì¶œ"
    prompt = f"""
ë‹¹ì‹ ì€ í‘œ/ê·¸ë¦¼ì„ ì„¤ëª…í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.
- [ì»¨í…ìŠ¤íŠ¸] ë²”ìœ„ ë‚´ì—ì„œë§Œ ë‹µí•˜ì„¸ìš”.
- ë‹µë³€ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ê°„ê²°íˆ.
- ì¶”ì„¸Â·ë¹„êµÂ·í•µì‹¬ í¬ì¸íŠ¸ë§Œ ì„¤ëª…í•˜ì„¸ìš”.

[ì§ˆë¬¸]
{query}

[ì»¨í…ìŠ¤íŠ¸]
{ctx}
""".strip()

    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": SUMMARIZER_DEFAULT_SYSTEM},
                {"role": "user", "content": prompt},
            ],
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"âš ï¸ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}"


# -------------------------------------------------------------------
# ğŸ–¼ï¸ ê·¸ë¦¼ ìš”ì•½
# -------------------------------------------------------------------
def explain_figure_image(query: str, image: Union[bytes, "PILImage.Image", Any],
                         neighbor_text: str = "", model_name: str = "gpt-4o-mini") -> str:
    client = _get_openai_client()
    prompt = f"""
[ì§ˆë¬¸]
{query}

[ì°¸ê³  ë³¸ë¬¸]
{(neighbor_text or '')[:1500]}
""".strip()
    try:
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ì‹œê°í™”ë¥¼ ì •í™•íˆ ì½ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt},
            ],
        )
        return resp.choices[0].message.content.strip()
    except Exception:
        return answer_with_context(
            query,
            f"[ì´ë¯¸ì§€ ìš”ì•½ ì‹¤íŒ¨: ë©€í‹°ëª¨ë‹¬ í˜¸ì¶œ ì˜ˆì™¸] ì•„ë˜ ë³¸ë¬¸ë§Œìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.\n{(neighbor_text or '')[:1800]}",
            page_label=None,
        )
