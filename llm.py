# llm.py
# ==========================================
# Gemini LLM ëª¨ë“ˆ (ìµœì¢… í†µí•©ë³¸)
# - Streamlit Cloud Secrets ìš°ì„  â†’ í™˜ê²½ë³€ìˆ˜(os.getenv) ë³´ì¡°
# - import ì‹œì ì— RuntimeError ë°œìƒí•˜ì§€ ì•Šë„ë¡ Lazy Init ì ìš©
# - summarizer.py í˜¸í™˜: llm_chat, SUMMARIZER_DEFAULT_SYSTEM ì¶”ê°€
# ==========================================

import os
import streamlit as st
import google.generativeai as genai


# -----------------------------
# ğŸ”‘ API í‚¤ ê°€ì ¸ì˜¤ê¸°
# -----------------------------
def _get_api_key():
    # 1) Streamlit Secrets
    key = None
    try:
        key = st.secrets.get("GEMINI_API_KEY", None)  # type: ignore
    except Exception:
        pass

    # 2) í™˜ê²½ë³€ìˆ˜
    if not key:
        key = os.getenv("GEMINI_API_KEY")

    return key


# -----------------------------
# âš™ï¸ Gemini ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
# -----------------------------
def _get_model(model_name: str = "gemini-1.5-flash"):
    api_key = _get_api_key()
    if not api_key:
        # Streamlit UIì—ì„œ ì—ëŸ¬ ì¶œë ¥
        st.error(
            "âŒ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
            "ğŸ‘‰ Streamlit Cloud Settings â†’ Secretsì—\n"
            '   GEMINI_API_KEY = "ë°œê¸‰ë°›ì€_API_KEY"\n'
            "í˜•ì‹ìœ¼ë¡œ ì €ì¥í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."
        )
        st.stop()

    # Gemini SDK ì´ˆê¸°í™”
    genai.configure(api_key=api_key)
    return genai.GenerativeModel(model_name)


# -----------------------------
# Provider ì´ë¦„ ë°˜í™˜
# -----------------------------
def get_provider_name() -> str:
    return "GEMINI"


# -----------------------------
# Summarizer ê¸°ë³¸ System í”„ë¡¬í”„íŠ¸
# -----------------------------
SUMMARIZER_DEFAULT_SYSTEM = """ë‹¹ì‹ ì€ ì •ì±…/ë³´ê³ ì„œ ì „ë¬¸ ìš”ì•½ê°€ì…ë‹ˆë‹¤.
- ë¬¸ì„œ ë°– ì •ë³´ëŠ” ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
"""


# -----------------------------
# ê³µìš© LLM í˜¸ì¶œ í•¨ìˆ˜
# -----------------------------
def llm_chat(system_prompt: str, user_prompt: str, model_name: str = "gemini-1.5-flash") -> str:
    """
    system_prompt: ì—­í• /ê·œì¹™ ì§€ì •
    user_prompt: ì‹¤ì œ ìš”ì•½/ì§ˆì˜ ë‚´ìš©
    """
    model = _get_model(model_name)
    try:
        resp = model.generate_content(
            [
                {"role": "system", "parts": [system_prompt]},
                {"role": "user", "parts": [user_prompt]},
            ]
        )
        return getattr(resp, "text", "").strip() or "âš ï¸ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"âš ï¸ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}"


# -----------------------------
# ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€
# -----------------------------
def answer_with_context(query: str, context: str) -> str:
    """
    query: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸
    context: ë¬¸ì„œ/ë°ì´í„°ì—ì„œ ë½‘ì•„ì˜¨ ê´€ë ¨ ë¬¸ë§¥
    """
    model = _get_model()

    prompt = f"""
ì•„ë˜ ë¬¸ë§¥(context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸(query)ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.
- ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ "ë¬¸ë§¥ì— ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.
- ìˆ«ìë‚˜ í‘œëŠ” í•µì‹¬ë§Œ ìš”ì•½í•˜ì„¸ìš”.

[Context]
{context}

[Question]
{query}
"""

    try:
        response = model.generate_content(prompt)
        return getattr(response, "text", "").strip() or "âš ï¸ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"âš ï¸ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}"


# -----------------------------
# í…Œì´ë¸” ì„¤ëª…
# -----------------------------
def explain_tables(table_text: str) -> str:
    """
    table_text: PDF ë“±ì—ì„œ ì¶”ì¶œëœ í…Œì´ë¸” ë¬¸ìì—´
    """
    model = _get_model()

    prompt = f"""
ì•„ë˜ í‘œ ë°ì´í„°ë¥¼ ì½ê³ , ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”:
1) í‘œì˜ ì£¼ì œ
2) ëˆˆì— ë„ëŠ” ì¶”ì„¸(ì¦ê°€/ê°ì†Œ)
3) ì¤‘ìš”í•œ ìˆ˜ì¹˜ 1~2ê°œ
4) ì •ì±…/ì˜ì‚¬ê²°ì • ì‹œì‚¬ì 

[Table]
{table_text}
"""

    try:
        response = model.generate_content(prompt)
        return getattr(response, "text", "").strip() or "âš ï¸ ì‘ë‹µì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
    except Exception as e:
        return f"âš ï¸ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}"
